{"cells":[{"cell_type":"markdown","source":["### Clean out stage table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f43c9d13-1e3e-4b63-b7af-90f4a79b198e"}}},{"cell_type":"code","source":["%sql\n\nTRUNCATE TABLE dataops_sandbox.tablemetricsstage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36de6678-90cf-4221-ae08-cef0f1c30dec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Check Access & Generate Data Tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2e2ffdc-ab4a-44bc-ba3f-d80da8af1127"}}},{"cell_type":"code","source":["# Imports\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.utils import ParseException\nfrom pyspark.sql.utils import IllegalArgumentException\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, LongType, DateType\nimport pyspark.sql.functions as f\nimport functools\n\n# Config\n# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n# -------------------------------------------------------------------------------------------------------------------------------\ndftbls = spark.sql(\"show tables\")\ndfdbs = spark.sql(\"show databases\")\n\n# Only want metadata from these listed databases below, make sure to comment out if statement if you want just that DB\ndatabaselist = []\n\n# Create DataFrame, assign columns, and puts metadata list into it\nschema = StructType([StructField(\"database\", StringType(), True), StructField(\"tableName\", StringType(), True),StructField(\"rowCount\", LongType(), True),\n                    StructField(\"format\", StringType(), True),StructField(\"numFiles\", LongType(), True),\n                    StructField(\"sizeInBytes\", LongType(), True),StructField(\"lastModified\", TimestampType(), True),StructField(\"location\", StringType(), True),StructField(\"domainUDP\", StringType(), True),\n                    StructField(\"table\", StringType(), True),StructField(\"source\", StringType(), True),StructField(\"lastLoadDate\", DateType(), True),StructField(\"lastLoadCount\", LongType(), True),\n                    StructField(\"reportDate\", DateType(), True),StructField(\"s3_root_location\", StringType(), True),StructField(\"columnCount\", LongType(), True)])\ndf = spark.createDataFrame(data=spark.sparkContext.emptyRDD(),schema=schema)\n\n# Shows all tables in the databases above and puts them into a DataFrame\nfor row in dfdbs.rdd.collect():\n#   if row['databaseName'] in databaselist:\n    tmp = \"show tables from \" + row['databaseName']\n    dftbls = dftbls.union(spark.sql(tmp))\n\n# Loops through each table and finds your metadata, puts it into a list. Try/catch prevents code from crashing when it runs into a database you don't have access to.\ndatabaseTablePairs = []\nfor row in dftbls.rdd.collect():\n  databaseTablePairs.append([row['database'],row['tableName']])\n\n# Not using .collect() in loops anymore to force data to stay on nodes until the end\ndfList = []\ndf.createOrReplaceTempView('FinalTempView')\nfor database, tableName in databaseTablePairs:\n  print('Analyzing: ' + database + '.' + tableName)\n  if database != 'default':\n    # Craft DataFrames for each table metric we are looking for\n    # ----------------------\n    \n    # Row Counts\n#     print('Getting Row Counts for: ' + database + '.' + tableName)\n    try:\n      tmpcntdf = spark.sql('select count(*) rowcnt from ' + database + '.' + tableName)\n    except (AnalysisException):\n      tmpcntdf = spark.sql('select NULL rowcnt')\n    except Exception as e:\n      if 'java.io.FileNotFoundException' in str(e):\n        tmpcntdf = spark.sql('select NULL rowcnt')\n        \n    # Table Details\n#     print('Getting Table Details for: ' + database + '.' + tableName)\n    try:\n      tmpdeetsdf = spark.sql('DESCRIBE DETAIL ' + database + '.' + tableName)\n    except (AnalysisException):\n      tmpdeetsdf = spark.sql('SELECT NULL format, NULL id, NULL name, NULL description, NULL location, NULL createdAt, NULL lastModified, NULL partitionColumns,NULL numFiles, NULL sizeInBytes, NULL properties, NULL minReaderVersion, NULL minWriterVersion')\n    except Exception as e:\n      if 'java.io.FileNotFoundException' in str(e):\n        tmpdeetsdf = spark.sql('SELECT NULL format, NULL id, NULL name, NULL description, NULL location, NULL createdAt, NULL lastModified, NULL partitionColumns,NULL numFiles, NULL sizeInBytes, NULL properties, NULL minReaderVersion, NULL minWriterVersion')\n        \n    # UDP columns\n#     print('Parsing UDP columns from: ' + database + '.' + tableName)\n    try:\n      tmpsourcedf = spark.sql('SELECT DISTINCT domain domainUDP, table, source FROM ' + database + '.' + tableName + ' LIMIT 1')\n    except (AnalysisException):\n      tmpsourcedf = spark.sql('SELECT NULL domainUDP, NULL table, NULL source')\n    except Exception as e:\n      if 'java.io.FileNotFoundException' in str(e):\n        tmpsourcedf = spark.sql('SELECT NULL domainUDP, NULL table, NULL source')\n        \n    # Last load dates and counts\n#     print('Getting Last Load Dates/Counts for: ' + database + '.' + tableName)\n    try:\n      tmploaddf = spark.sql('SELECT DISTINCT CAST(eventTimestamp AS DATE) lastLoadDate, COUNT(*) lastLoadCount from ' + database + '.' + tableName + ' GROUP by CAST(eventTimestamp AS DATE) ORDER BY lastLoadDate DESC LIMIT 1')\n    except (AnalysisException):\n      tmploaddf = spark.sql('SELECT NULL lastLoadDate, NULL lastLoadCount')\n    except Exception as e:\n      if 'java.io.FileNotFoundException' in str(e):\n        tmploaddf = spark.sql('SELECT NULL lastLoadDate, NULL lastLoadCount')\n        \n    # Report Date\n#     print('Getting Current Date')\n    currentdatedf = spark.sql('SELECT CAST(CURRENT_TIMESTAMP() AS DATE) reportDate')\n    \n    # S3 Root Location\n#     print('Getting S3 Root Location for: ' + database + '.' + tableName)\n    try:\n      tmps3df = spark.sql('DESCRIBE SCHEMA ' + database).filter(f.col('database_description_item') == 'Location').withColumnRenamed(existing='database_description_value',new='s3_root_location').select('s3_root_location')\n    except (AnalysisException):\n      tmps3df = spark.sql('SELECT NULL s3_root_location')\n      \n    try:\n      tmpcoldf = spark.sql('SELECT * from ' + database + '.' + tableName + ' LIMIT 1')\n      columncount = int(len(tmpcoldf.columns))\n    except (AnalysisException):\n      columncount = 0\n    except Exception as e:\n      if 'java.io.FileNotFoundException' in str(e):\n        columncount = 0\n\n# Join all the DataFrames\n#     print('Joining Data for: ' + database + '.' + tableName)\n    joineddf = tmpcntdf.join(tmpdeetsdf,how='full').join(tmpsourcedf,how='full').join(tmploaddf,how='full').join(currentdatedf,how='full').join(tmps3df,how='full').cache()\n#     print('Filtering Data for: ' + database + '.' + tableName)\n    filtereddf = joineddf.withColumn('database', f.lit(database)).withColumn('tableName', f.lit(tableName)).withColumn('columnCount', f.lit(columncount)).select('database','tableName',joineddf.rowcnt.alias('rowCount'),'format', 'numFiles', 'sizeInBytes','lastModified','location','domainUDP', 'table', 'source', 'lastLoadDate', 'lastLoadCount','reportDate','s3_root_location','columnCount').cache()\n    df_mb = filtereddf.withColumn('sizeInMB',f.round((f.col('sizeInBytes') / 1000000),2))\n    df_mb = df_mb.withColumn('domain',f.when(f.col('location').rlike('bronze'), f.split(f.col(\"location\"), \"/\").getItem(4)).otherwise(f.split(f.col(\"location\"), \"/\").getItem(3))).select('reportDate','domain', 'database','tableName','rowCount','format','lastLoadDate','lastLoadCount', 'numFiles', 'sizeInMB','lastModified','location','domainUDP','table','source','s3_root_location','columnCount')\n\n    df_mb.createOrReplaceTempView('TempView')\n    spark.sql('INSERT INTO dataops_sandbox.tablemetricsstage SELECT * FROM TempView')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a86f1ea5-0957-4e10-a93a-e0c8df516ae6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-269496&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">116</span>\n<span class=\"ansi-red-fg\">    spark.sql(&#39;INSERT INTO dataops_sandbox.tablemetricsstage SELECT * FROM TempView WHERE col_name IS NOT NULL OR col_name != &#39;&#39;)</span>\n                                                                                                                                 ^\n<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> EOL while scanning string literal\n</div>","errorSummary":"<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> EOL while scanning string literal","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-269496&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">116</span>\n<span class=\"ansi-red-fg\">    spark.sql(&#39;INSERT INTO dataops_sandbox.tablemetricsstage SELECT * FROM TempView WHERE col_name IS NOT NULL OR col_name != &#39;&#39;)</span>\n                                                                                                                                 ^\n<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> EOL while scanning string literal\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Delete Data if there already is some in there for today"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28fe2c6c-b654-4e9e-bebe-271070646148"}}},{"cell_type":"code","source":["%sql\n\nDELETE FROM dataops_sandbox.tablemetrics WHERE reportDate = CAST(CURRENT_TIMESTAMP() AS DATE)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41c91e0e-eaa8-49a5-be12-ed77a5151f9d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Insert Daily Data into Delta Table from Stage Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6aece692-fa9e-4629-8014-36ca84a887ba"}}},{"cell_type":"code","source":["%sql\nINSERT INTO dataops_sandbox.tablemetrics SELECT * FROM dataops_sandbox.tablemetricsstage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78dcf8f2-aa54-4fff-acfa-85e765eb31fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Clean Out Stage Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"254425ff-c136-4e68-9919-77c8a32f0235"}}},{"cell_type":"code","source":["%sql\nTRUNCATE TABLE dataops_sandbox.tablemetricsstage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54f698de-d4a9-4177-89f7-3fe029b9e6f4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Zone Table Creation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c5bc2da-a22d-48c1-91c0-7c9c65cae7eb"}}},{"cell_type":"code","source":["df = spark.sql('''SELECT database,\ntableName,\nCOALESCE(location, s3_root_location) AS s3_location,\nCASE WHEN COALESCE(location, s3_root_location) LIKE '%sandbox%' THEN 'Sandbox'\nWHEN COALESCE(location, s3_root_location) LIKE '%/refined/%' THEN 'Refined'\nWHEN COALESCE(location, s3_root_location) LIKE '%/structured/%' THEN 'Structured Raw'\nWHEN COALESCE(location, s3_root_location) LIKE '%/bronze/%' THEN 'Bronze'\nWHEN COALESCE(location, s3_root_location) LIKE '%/hive/warehouse%' THEN 'Hive Warehouse/unknown'\nWHEN COALESCE(location, s3_root_location) LIKE '%/client/BIIPODS%' THEN 'Structured Raw'\nWHEN COALESCE(location, s3_root_location) LIKE '%/px/px_landing/%' THEN 'Structured Raw'\nWHEN COALESCE(location, s3_root_location) LIKE '%/source/px/%' THEN 'Structured Raw'\nELSE 'Sandbox'\nEND AS Zone \nFROM dataops_sandbox.tablemetrics WHERE reportDate = (SELECT DISTINCT reportDate FROm dataops_sandbox.tablemetrics ORDER BY reportDate DESC LIMIT 1)''')\n\ndf.write.insertInto(\"dataops_sandbox.tables_by_zone\", overwrite=True)\n\ndisplay(df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"822d878a-dfd3-4547-94cb-ba00a55f42c5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Domain Table Creations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e66d2d2f-b2aa-4ce9-b413-8160f5b2d384"}}},{"cell_type":"code","source":["df_domains = spark.sql('''SELECT DISTINCT COALESCE(domain,domainUDP) AS domain, database, tableName from dataops_sandbox.tablemetrics WHERE reportDate = (SELECT DISTINCT reportDate FROm dataops_sandbox.tablemetrics ORDER BY reportDate DESC LIMIT 1)''')\ndf_domains.write.insertInto(\"dataops_sandbox.tables_by_domain\", overwrite=True)\n\ndisplay(df_domains)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8bfdc9a6-98f9-43e5-9af4-f9c39c3e0464"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Column Count Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02e18767-b8e6-4969-b3c3-ff18f5c0a728"}}},{"cell_type":"code","source":["df_columns = spark.sql('''SELECT DISTINCT columnCount AS columnCount, database, tableName from dataops_sandbox.tablemetrics WHERE reportDate = (SELECT DISTINCT reportDate FROm dataops_sandbox.tablemetrics ORDER BY reportDate DESC LIMIT 1)''')\n# df_columns.write.format(\"delta\").saveAsTable('dataops_sandbox.columnCount_by_table')\ndf_columns.write.insertInto(\"dataops_sandbox.columnCount_by_table\", overwrite=True)\n\ndisplay(df_columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"387c4b0e-3e6b-4ab0-83f3-9ebe10496815"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Optimization Candidate Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8553c23-9412-4060-8992-a5c64cc0304c"}}},{"cell_type":"code","source":["df_optimize = spark.sql('''SELECT reportDate, database, tableName, numFiles FROM dataops_sandbox.tablemetrics WHERE CAST(numFiles AS INT) > 1000 and reportDate = (SELECT MAX(reportDate) FROM dataops_sandbox.tablemetrics) ORDER BY CAST(numFiles AS INT) DESC''')\n\ndf_optimize.write.insertInto(\"dataops_sandbox.optimizationcandidates\", overwrite=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7819e9a6-73e5-4949-abd9-a77cbaef5327"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Optimize raw table every so often"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b576b5c-2b46-495d-9958-a904f528e67a"}}},{"cell_type":"code","source":["df_opt = spark.sql('''DESCRIBE DETAIL dataops_sandbox.tablemetrics''')\n\ndisplay(df_opt)\n\nif df_opt.select('numFiles').collect()[0][0] > 300:\n  spark.sql('''OPTIMIZE dataops_sandbox.tablemetrics''')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b99d93ee-6eca-4b31-baf5-31c738086190"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"UDP Metrics","dashboards":[{"elements":[],"guid":"c967d4e9-814d-4302-8ba8-b3ad8f9b7af6","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"87632033-36a5-471d-9865-2e8e393a68e1","origId":269520,"title":"Untitled","width":1600,"globalVars":{}}],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":233604}},"nbformat":4,"nbformat_minor":0}
