{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f43c9d13-1e3e-4b63-b7af-90f4a79b198e","showTitle":false,"title":""}},"source":["### Clean out stage table"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"36de6678-90cf-4221-ae08-cef0f1c30dec","showTitle":false,"title":""},"vscode":{"languageId":"sql"}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{"isDbfsCommandResult":false},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[],"type":"table"}},"output_type":"display_data"}],"source":["%sql\n","\n","TRUNCATE TABLE dataops_sandbox.tablemetricsstage"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f2e2ffdc-ab4a-44bc-ba3f-d80da8af1127","showTitle":false,"title":""}},"source":["### Check Access & Generate Data Tables"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a86f1ea5-0957-4e10-a93a-e0c8df516ae6","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"},{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-269496&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">116</span>\n","<span class=\"ansi-red-fg\">    spark.sql(&#39;INSERT INTO dataops_sandbox.tablemetricsstage SELECT * FROM TempView WHERE col_name IS NOT NULL OR col_name != &#39;&#39;)</span>\n","                                                                                                                                 ^\n","<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> EOL while scanning string literal\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-269496&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">116</span>\n<span class=\"ansi-red-fg\">    spark.sql(&#39;INSERT INTO dataops_sandbox.tablemetricsstage SELECT * FROM TempView WHERE col_name IS NOT NULL OR col_name != &#39;&#39;)</span>\n                                                                                                                                 ^\n<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> EOL while scanning string literal\n</div>","errorSummary":"<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> EOL while scanning string literal","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["# Imports\n","from pyspark.sql.utils import AnalysisException\n","from pyspark.sql.utils import ParseException\n","from pyspark.sql.utils import IllegalArgumentException\n","from pyspark.sql import DataFrame\n","from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, LongType, DateType\n","import pyspark.sql.functions as f\n","import functools\n","\n","# Config\n","# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n","# ------------------------------------------------------------------------------------------------------------------------------\n","dfdbs = spark.sql(\"show databases\")\n","\n","# Only want metadata from these listed databases below, make sure to comment out if statement if you want just that DB\n","databaselist = []\n","\n","# Create DataFrame, assign columns, and puts metadata list into it\n","schema = StructType([StructField(\"database\", StringType(), True), StructField(\"tableName\", StringType(), True),StructField(\"rowCount\", LongType(), True),\n","                    StructField(\"format\", StringType(), True),StructField(\"numFiles\", LongType(), True),\n","                    StructField(\"sizeInBytes\", LongType(), True),StructField(\"lastModified\", TimestampType(), True),StructField(\"location\", StringType(), True),StructField(\"domainUDP\", StringType(), True),\n","                    StructField(\"table\", StringType(), True),StructField(\"source\", StringType(), True),StructField(\"lastLoadDate\", DateType(), True),StructField(\"lastLoadCount\", LongType(), True),\n","                    StructField(\"reportDate\", DateType(), True),StructField(\"s3_root_location\", StringType(), True),StructField(\"columnCount\", LongType(), True)])\n","df = spark.createDataFrame(data=spark.sparkContext.emptyRDD(),schema=schema)\n","\n","# Shows all tables in the databases above and puts them into a DataFrame\n","for row in dfdbs.rdd.collect():\n","#   if row['databaseName'] in databaselist:\n","    tmp = \"show tables from \" + row['databaseName']\n","    dftbls = dftbls.union(spark.sql(tmp))\n","\n","# Loops through each table and finds your metadata, puts it into a list. Try/catch prevents code from crashing when it runs into a database you don't have access to.\n","\n","# Not using .collect() in for loop anymore to force data to stay on nodes until the end\n","databaseTablePairs = []\n","for row in dftbls.rdd.collect():\n","  databaseTablePairs.append([row['database'],row['tableName']])\n","\n","for database, tableName in databaseTablePairs:\n","  print('Analyzing: ' + database + '.' + tableName)\n","  if database != 'default':\n","    # Craft DataFrames for each table metric we are looking for\n","    # ------------------------------------------------------------\n","    \n","    # Row Counts\n","    try:\n","      tmpcntdf = spark.sql('select count(*) rowcnt from ' + database + '.' + tableName)\n","    except (AnalysisException):\n","      tmpcntdf = spark.sql('select NULL rowcnt')\n","    except Exception as e:\n","      if 'java.io.FileNotFoundException' in str(e):\n","        tmpcntdf = spark.sql('select NULL rowcnt')\n","        \n","    # Table Details\n","    try:\n","      tmpdeetsdf = spark.sql('DESCRIBE DETAIL ' + database + '.' + tableName)\n","    except (AnalysisException):\n","      tmpdeetsdf = spark.sql('SELECT NULL format, NULL id, NULL name, NULL description, NULL location, NULL createdAt, NULL lastModified, NULL partitionColumns,NULL numFiles, NULL sizeInBytes, NULL properties, NULL minReaderVersion, NULL minWriterVersion')\n","    except Exception as e:\n","      if 'java.io.FileNotFoundException' in str(e):\n","        tmpdeetsdf = spark.sql('SELECT NULL format, NULL id, NULL name, NULL description, NULL location, NULL createdAt, NULL lastModified, NULL partitionColumns,NULL numFiles, NULL sizeInBytes, NULL properties, NULL minReaderVersion, NULL minWriterVersion')\n","        \n","    # UDP columns\n","    try:\n","      tmpsourcedf = spark.sql('SELECT DISTINCT domain domainUDP, table, source FROM ' + database + '.' + tableName + ' LIMIT 1')\n","    except (AnalysisException):\n","      tmpsourcedf = spark.sql('SELECT NULL domainUDP, NULL table, NULL source')\n","    except Exception as e:\n","      if 'java.io.FileNotFoundException' in str(e):\n","        tmpsourcedf = spark.sql('SELECT NULL domainUDP, NULL table, NULL source')\n","        \n","    # Last load dates and counts\n","    try:\n","      tmploaddf = spark.sql('SELECT DISTINCT CAST(eventTimestamp AS DATE) lastLoadDate, COUNT(*) lastLoadCount from ' + database + '.' + tableName + ' GROUP by CAST(eventTimestamp AS DATE) ORDER BY lastLoadDate DESC LIMIT 1')\n","    except (AnalysisException):\n","      tmploaddf = spark.sql('SELECT NULL lastLoadDate, NULL lastLoadCount')\n","    except Exception as e:\n","      if 'java.io.FileNotFoundException' in str(e):\n","        tmploaddf = spark.sql('SELECT NULL lastLoadDate, NULL lastLoadCount')\n","        \n","    # Report Date\n","    currentdatedf = spark.sql('SELECT CAST(CURRENT_TIMESTAMP() AS DATE) reportDate')\n","    \n","    # S3 Root Location\n","    try:\n","      tmps3df = spark.sql('DESCRIBE SCHEMA ' + database).filter(f.col('database_description_item') == 'Location').withColumnRenamed(existing='database_description_value',new='s3_root_location').select('s3_root_location')\n","    except (AnalysisException):\n","      tmps3df = spark.sql('SELECT NULL s3_root_location')\n","    \n","    # Special UDP Columns\n","    try:\n","      tmpcoldf = spark.sql('SELECT * from ' + database + '.' + tableName + ' LIMIT 1')\n","      columncount = int(len(tmpcoldf.columns))\n","    except (AnalysisException):\n","      columncount = 0\n","    except Exception as e:\n","      if 'java.io.FileNotFoundException' in str(e):\n","        columncount = 0\n","\n","# Join all the DataFrames\n","    joineddf = tmpcntdf.join(tmpdeetsdf,how='full').join(tmpsourcedf,how='full').join(tmploaddf,how='full').join(currentdatedf,how='full').join(tmps3df,how='full').cache()\n","    filtereddf = joineddf.withColumn('database', f.lit(database)).withColumn('tableName', f.lit(tableName)).withColumn('columnCount', f.lit(columncount)).select('database','tableName',joineddf.rowcnt.alias('rowCount'),'format', 'numFiles', 'sizeInBytes','lastModified','location','domainUDP', 'table', 'source', 'lastLoadDate', 'lastLoadCount','reportDate','s3_root_location','columnCount').cache()\n","    df_mb = filtereddf.withColumn('sizeInMB',f.round((f.col('sizeInBytes') / 1000000),2))\n","\n","    # Parse for domain from s3 location, special case for bronze s3 locations\n","    df_mb = df_mb.withColumn('domain',f.when(f.col('location').rlike('bronze'), f.split(f.col(\"location\"), \"/\").getItem(4)).otherwise(f.split(f.col(\"location\"), \"/\").getItem(3))).select('reportDate','domain', 'database','tableName','rowCount','format','lastLoadDate','lastLoadCount', 'numFiles', 'sizeInMB','lastModified','location','domainUDP','table','source','s3_root_location','columnCount')\n","\n","    df_mb.createOrReplaceTempView('TempView')\n","    spark.sql('INSERT INTO dataops_sandbox.tablemetricsstage SELECT * FROM TempView')"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"28fe2c6c-b654-4e9e-bebe-271070646148","showTitle":false,"title":""}},"source":["### Delete Data if there already is some in there for today"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"41c91e0e-eaa8-49a5-be12-ed77a5151f9d","showTitle":false,"title":""},"vscode":{"languageId":"sql"}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["%sql\n","\n","DELETE FROM dataops_sandbox.tablemetrics WHERE reportDate = CAST(CURRENT_TIMESTAMP() AS DATE)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6aece692-fa9e-4629-8014-36ca84a887ba","showTitle":false,"title":""}},"source":["### Insert Daily Data into Delta Table from Stage Table"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"78dcf8f2-aa54-4fff-acfa-85e765eb31fa","showTitle":false,"title":""},"vscode":{"languageId":"sql"}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["%sql\n","INSERT INTO dataops_sandbox.tablemetrics SELECT * FROM dataops_sandbox.tablemetricsstage"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"254425ff-c136-4e68-9919-77c8a32f0235","showTitle":false,"title":""}},"source":["### Clean Out Stage Table"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"54f698de-d4a9-4177-89f7-3fe029b9e6f4","showTitle":false,"title":""},"vscode":{"languageId":"sql"}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["%sql\n","TRUNCATE TABLE dataops_sandbox.tablemetricsstage"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1c5bc2da-a22d-48c1-91c0-7c9c65cae7eb","showTitle":false,"title":""}},"source":["### Zone Table Creation"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"822d878a-dfd3-4547-94cb-ba00a55f42c5","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["df = spark.sql('''SELECT database, tableName,\n","COALESCE(location, s3_root_location) AS s3_location,\n","CASE WHEN COALESCE(location, s3_root_location) LIKE '%sandbox%' THEN 'Sandbox'\n","WHEN COALESCE(location, s3_root_location) LIKE '%/refined/%' THEN 'Refined'\n","WHEN COALESCE(location, s3_root_location) LIKE '%/structured/%' THEN 'Structured Raw'\n","WHEN COALESCE(location, s3_root_location) LIKE '%/bronze/%' THEN 'Bronze'\n","WHEN COALESCE(location, s3_root_location) LIKE '%/hive/warehouse%' THEN 'Hive Warehouse/unknown'\n","WHEN COALESCE(location, s3_root_location) LIKE '%/client/BIIPODS%' THEN 'Structured Raw'\n","WHEN COALESCE(location, s3_root_location) LIKE '%/px/px_landing/%' THEN 'Structured Raw'\n","WHEN COALESCE(location, s3_root_location) LIKE '%/source/px/%' THEN 'Structured Raw'\n","ELSE 'Sandbox'\n","END AS Zone \n","FROM dataops_sandbox.tablemetrics WHERE reportDate = (SELECT DISTINCT reportDate FROm dataops_sandbox.tablemetrics ORDER BY reportDate DESC LIMIT 1)''')\n","\n","df.write.insertInto(\"dataops_sandbox.tables_by_zone\", overwrite=True)\n","\n","display(df)\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e66d2d2f-b2aa-4ce9-b413-8160f5b2d384","showTitle":false,"title":""}},"source":["### Domain Table Creations"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8bfdc9a6-98f9-43e5-9af4-f9c39c3e0464","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["df_domains = spark.sql('''SELECT DISTINCT COALESCE(domain,domainUDP) AS domain, database, tableName from dataops_sandbox.tablemetrics WHERE reportDate = (SELECT DISTINCT reportDate FROm dataops_sandbox.tablemetrics ORDER BY reportDate DESC LIMIT 1)''')\n","df_domains.write.insertInto(\"dataops_sandbox.tables_by_domain\", overwrite=True)\n","\n","display(df_domains)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"02e18767-b8e6-4969-b3c3-ff18f5c0a728","showTitle":false,"title":""}},"source":["### Column Count Table"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"387c4b0e-3e6b-4ab0-83f3-9ebe10496815","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["df_columns = spark.sql('''SELECT DISTINCT columnCount AS columnCount, database, tableName from dataops_sandbox.tablemetrics \n","WHERE reportDate = (SELECT DISTINCT reportDate FROm dataops_sandbox.tablemetrics ORDER BY reportDate DESC LIMIT 1)''')\n","# df_columns.write.format(\"delta\").saveAsTable('dataops_sandbox.columnCount_by_table')\n","df_columns.write.insertInto(\"dataops_sandbox.columnCount_by_table\", overwrite=True)\n","\n","display(df_columns)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c8553c23-9412-4060-8992-a5c64cc0304c","showTitle":false,"title":""}},"source":["### Optimization Candidate Table"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7819e9a6-73e5-4949-abd9-a77cbaef5327","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["df_optimize = spark.sql('''SELECT reportDate, database, tableName, numFiles FROM dataops_sandbox.tablemetrics WHERE CAST(numFiles AS INT) > 1000 \n","and reportDate = (SELECT MAX(reportDate) FROM dataops_sandbox.tablemetrics) ORDER BY CAST(numFiles AS INT) DESC''')\n","\n","df_optimize.write.insertInto(\"dataops_sandbox.optimizationcandidates\", overwrite=True)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6b576b5c-2b46-495d-9958-a904f528e67a","showTitle":false,"title":""}},"source":["### Optimize raw table every so often"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b99d93ee-6eca-4b31-baf5-31c738086190","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"Command skipped","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["df_opt = spark.sql('''DESCRIBE DETAIL dataops_sandbox.tablemetrics''')\n","\n","display(df_opt)\n","\n","if df_opt.select('numFiles').collect()[0][0] > 300:\n","  spark.sql('''OPTIMIZE dataops_sandbox.tablemetrics''')"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[{"elements":[],"globalVars":{},"guid":"c967d4e9-814d-4302-8ba8-b3ad8f9b7af6","layoutOption":{"grid":true,"stack":true},"nuid":"87632033-36a5-471d-9865-2e8e393a68e1","origId":269520,"title":"Untitled","version":"DashboardViewV1","width":1600}],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"UDP Metrics","notebookOrigID":233604,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
